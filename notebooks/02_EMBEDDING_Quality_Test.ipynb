{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Quality Test\n",
    "\n",
    "Test embedding models and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from src.indexing.embedding_models import HuggingFaceEmbedder, CachedEmbedder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedder\n",
    "embedder = CachedEmbedder(model_name=\"all-MiniLM-L6-v2\", cache_dir=Path(\"./data/cache\"))\n",
    "\n",
    "print(f\"Embedding dimension: {embedder.get_embedding_dimension()}\")\n",
    "print(f\"Model: {embedder.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic similarity\n",
    "sentences = [\n",
    "    \"The attention mechanism improves transformer performance\",\n",
    "    \"Transformers use attention to weight different tokens\",\n",
    "    \"Deep learning models require large datasets\",\n",
    "    \"Neural networks have many parameters\",\n",
    "]\n",
    "\n",
    "# Embed\n",
    "embeddings = embedder.embed_documents(sentences)\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Compute similarities\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"Semantic Similarity Matrix:\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Find most similar pair\n",
    "max_sim = 0\n",
    "max_pair = (0, 1)\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        if similarity_matrix[i][j] > max_sim:\n",
    "            max_sim = similarity_matrix[i][j]\n",
    "            max_pair = (i, j)\n",
    "\n",
    "print(f\"\\nMost similar:\")\n",
    "print(f\"  1. {sentences[max_pair[0]]}\")\n",
    "print(f\"  2. {sentences[max_pair[1]]}\")\n",
    "print(f\"  Similarity: {max_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "test_text = \"Testing cache performance with repeated embeddings\"\n",
    "\n",
    "# First embedding (not cached)\n",
    "start = time.time()\n",
    "emb1 = embedder.embed_query(test_text)\n",
    "time_first = time.time() - start\n",
    "\n",
    "# Second embedding (cached)\n",
    "start = time.time()\n",
    "emb2 = embedder.embed_query(test_text)\n",
    "time_second = time.time() - start\n",
    "\n",
    "print(f\"First call: {time_first*1000:.2f}ms\")\n",
    "print(f\"Second call (cached): {time_second*1000:.2f}ms\")\n",
    "print(f\"Speedup: {time_first/time_second:.1f}x\")\n",
    "print(f\"Cache working: {emb1 == emb2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
