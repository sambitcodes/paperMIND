{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Playground\n",
    "\n",
    "This notebook helps iterate on prompts:\n",
    "- Inspect system prompt used per GROQ model\n",
    "- Run the same question with different prompt constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from src.llm.prompts import get_groq_system_prompt\n",
    "from src.llm.orchestrator import LLMOrchestrator\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orch = LLMOrchestrator()\n",
    "models = orch.get_available_models()\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [\"llama-3.3-70b-versatile\", \"mixtral-8x7b-32768\", \"gemma-7b-it\"]:\n",
    "    if m in models:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MODEL:\", m)\n",
    "        print(\"=\"*80)\n",
    "        print(get_groq_system_prompt(m, has_context=True)[:1200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run prompt experiments\n",
    "\n",
    "Use a short synthetic context to validate grounding behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the main dataset used?\"\n",
    "context = \"\"\"The paper evaluates the method on the XYZ-1 benchmark dataset.\\nIt reports accuracy and F1 score on the validation split.\"\"\"\n",
    "\n",
    "candidate = [m for m in [\"llama-3.3-70b-versatile\", \"mixtral-8x7b-32768\", \"gemma-7b-it\"] if m in models]\n",
    "candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in candidate:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL:\", m)\n",
    "    print(\"=\"*80)\n",
    "    out = orch.generate(\n",
    "        model_name=m,\n",
    "        query=question,\n",
    "        context=context,\n",
    "        temperature=0.0,\n",
    "        max_tokens=256,\n",
    "        stream=False,\n",
    "    )\n",
    "    print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
