{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "Compares responses across:\n",
    "- GROQ generative models (if GROQ_API_KEY is set)\n",
    "- Extractive model (`scibert-extractive`) using the same retrieved context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from src.config import settings\n",
    "from src.indexing.index_manager import IndexManager\n",
    "from src.retrieval.retriever import RAGRetriever\n",
    "from src.llm.orchestrator import LLMOrchestrator\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./data/sample_papers/sample_paper.pdf\"\n",
    "collection_name = \"sample_paper_collection\"\n",
    "question = \"What are the main contributions of the paper?\"\n",
    "\n",
    "index_manager = IndexManager(\n",
    "    embedding_model=settings.EMBEDDING_MODEL,\n",
    "    chunk_size=settings.CHUNK_SIZE,\n",
    "    chunk_overlap=settings.CHUNK_OVERLAP,\n",
    "    persist_dir=settings.CHROMA_PERSIST_DIR,\n",
    ")\n",
    "\n",
    "if Path(pdf_path).exists():\n",
    "    index_manager.index_pdf(pdf_path, collection_name=collection_name)\n",
    "\n",
    "vector_store = index_manager.get_vector_store()\n",
    "embedder = index_manager.get_embedder()\n",
    "vector_store.create_collection(collection_name)\n",
    "\n",
    "retriever = RAGRetriever(\n",
    "    vector_store=vector_store,\n",
    "    embedder=embedder,\n",
    "    top_k=5,\n",
    "    similarity_threshold=settings.SIMILARITY_THRESHOLD,\n",
    ")\n",
    "\n",
    "retrieval = retriever.retrieve(question)\n",
    "context = retrieval.get_context()\n",
    "sources = retrieval.get_citations()\n",
    "\n",
    "print(\"Sources:\")\n",
    "for s in sources:\n",
    "    print(\"-\", s)\n",
    "\n",
    "print(\"\\nContext preview:\")\n",
    "print(context[:500] + (\"...\" if len(context) > 500 else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orch = LLMOrchestrator()\n",
    "models = orch.get_available_models()\n",
    "print(\"Available models:\", models)\n",
    "\n",
    "# Compare only a small subset if many are present\n",
    "candidate_models = []\n",
    "for m in [\"llama-3.3-70b-versatile\", \"mixtral-8x7b-32768\", \"gemma-7b-it\", \"scibert-extractive\"]:\n",
    "    if m in models:\n",
    "        candidate_models.append(m)\n",
    "\n",
    "print(\"Candidate models:\", candidate_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in candidate_models:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL:\", m)\n",
    "    print(\"=\"*80)\n",
    "    out = orch.generate(\n",
    "        model_name=m,\n",
    "        query=question,\n",
    "        context=context,\n",
    "        temperature=0.2,\n",
    "        max_tokens=512,\n",
    "        stream=False,\n",
    "    )\n",
    "    print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
